# ==================== Import Required Libraries ====================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.feature_selection import RFECV
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.pipeline import Pipeline
import os
import seaborn as sns

# Set font for plots to ensure consistent English rendering
plt.rcParams['font.family'] = 'Arial'

# Set working directory (adjust path as needed)
os.chdir("your own")

# ==================== Load and Prepare Data ====================
# Load dataset from CSV file
df = pd.read_csv('your own.csv')

# Extract features (all columns except the first) and labels (first column)
X = df.iloc[:, 1:].values          # Feature matrix (samples × features)
y = df.iloc[:, 0].values           # Target vector (e.g., 'Disease' vs 'Control')
feature_names = df.columns[1:]     # Names of all features

# ==================== Data Preprocessing ====================
# Standardize features to zero mean and unit variance (important for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Define stratified 5-fold cross-validation to maintain class distribution in each fold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ==================== Initialize SVM Classifier ====================
# Use linear kernel SVM for compatibility with feature ranking via coefficients
svm = SVC(kernel='linear', probability=True, random_state=42)

# ==================== Perform Recursive Feature Elimination with CV (RFECV) ====================
# RFECV automatically selects the optimal number of features by minimizing cross-validated error
rfecv = RFECV(
    estimator=svm,                 # Base estimator (must have coef_ or feature_importances_)
    step=1,                        # Remove 1 feature at each iteration
    cv=cv,                         # Cross-validation strategy
    scoring='accuracy',            # Metric to optimize
    min_features_to_select=1,      # Minimum number of features to consider
    n_jobs=-1,                     # Use all CPU cores for parallelization
    verbose=1                      # Print progress during execution
)

# Fit RFECV on the standardized data
print("Starting SVM-RFE feature selection...")
rfecv.fit(X_scaled, y)

# Retrieve the optimal number of features that minimizes CV error
n_features_optimal = rfecv.n_features_
print(f"Optimal number of features: {n_features_optimal}")

# Get feature rankings (1 = selected, higher = eliminated earlier)
feature_ranking = rfecv.ranking_
selected_features_mask = rfecv.support_  # Boolean mask of selected features

# Create a DataFrame summarizing feature importance and selection status
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Ranking': feature_ranking,
    'Selected': selected_features_mask
})

# Sort features by ranking (lowest rank = most important)
feature_importance_df = feature_importance_df.sort_values('Ranking')
print("\nFeatures sorted by importance:")
print(feature_importance_df)

# Save feature ranking results to CSV for further analysis
csv_path = os.path.join(os.getcwd(), "svm_rfe_feature_importance.csv")
feature_importance_df.to_csv(csv_path, index=False)
print(f"Feature importance saved to: {csv_path}")

# ==================== Extract and Process Cross-Validation Scores ====================
# Handle differences between scikit-learn versions for accessing CV scores
if hasattr(rfecv, 'cv_results_'):
    # Newer versions (≥1.0): use cv_results_['mean_test_score']
    cv_scores = rfecv.cv_results_['mean_test_score']
else:
    # Older versions: use deprecated grid_scores_
    cv_scores = rfecv.grid_scores_

# Compute error rate and accuracy across feature counts
error_rates = 1 - cv_scores
accuracy_rates = cv_scores

# Generate x-axis values: number of features evaluated (from 1 to total features)
n_total_features = len(feature_names)
feature_counts = np.arange(1, n_total_features + 1)[-len(cv_scores):]  # Align with CV score length

# ==================== Plot Error Rate vs. Number of Features ====================
plt.figure(figsize=(10, 8))
plt.plot(feature_counts, error_rates, 'o-', color='#E24A33', linewidth=2, markersize=8)

# Highlight the optimal number of features (lowest error)
plt.axvline(x=n_features_optimal, color='blue', linestyle='--', linewidth=2,
           label=f'Optimal Features: {n_features_optimal}')

# Labels and styling
plt.xlabel('Number of Features', fontsize=14)
plt.ylabel('Error Rate (1 - Accuracy)', fontsize=14)
plt.title('SVM-RFE: Error Rate vs Number of Features', fontsize=16)
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend(fontsize=12)
plt.tick_params(axis='both', which='major', labelsize=12)

# Annotate the point with minimum error
min_error_idx = np.argmin(error_rates)
min_error = error_rates[min_error_idx]
min_features = feature_counts[min_error_idx]
plt.plot(min_features, min_error, 'ro', markersize=10)
plt.annotate(f'Minimum Error Rate: {min_error:.4f}\nFeatures: {min_features}',
            xy=(min_features, min_error), 
            xytext=(min_features + 2, min_error + 0.05),
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"), 
            fontsize=12)

plt.tight_layout()

# Save plot in both PNG and PDF formats (high resolution for publication)
error_fig_path = os.path.join(os.getcwd(), "svm_rfe_error_rate.png")
plt.savefig(error_fig_path, dpi=300, bbox_inches='tight')
print(f"Error rate plot saved to: {error_fig_path}")

error_pdf_path = os.path.join(os.getcwd(), "svm_rfe_error_rate.pdf")
plt.savefig(error_pdf_path, format='pdf', dpi=300, bbox_inches='tight')
print(f"Error rate plot saved to PDF: {error_pdf_path}")

plt.show()

# ==================== Plot Accuracy vs. Number of Features ====================
plt.figure(figsize=(10, 8))
plt.plot(feature_counts, accuracy_rates, 'o-', color='#348ABD', linewidth=2, markersize=8)

# Highlight optimal feature count
plt.axvline(x=n_features_optimal, color='green', linestyle='--', linewidth=2,
           label=f'Optimal Features: {n_features_optimal}')

plt.xlabel('Number of Features', fontsize=14)
plt.ylabel('Accuracy', fontsize=14)
plt.title('SVM-RFE: Accuracy vs Number of Features', fontsize=16)
plt.grid(True, linestyle='--', alpha=0.7)
plt.legend(fontsize=12)
plt.tick_params(axis='both', which='major', labelsize=12)

# Annotate maximum accuracy point
max_acc_idx = np.argmax(accuracy_rates)
max_acc = accuracy_rates[max_acc_idx]
max_acc_features = feature_counts[max_acc_idx]
plt.plot(max_acc_features, max_acc, 'go', markersize=10)
plt.annotate(f'Maximum Accuracy: {max_acc:.4f}\nFeatures: {max_acc_features}',
            xy=(max_acc_features, max_acc), 
            xytext=(max_acc_features + 2, max_acc - 0.05),
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"), 
            fontsize=12)

plt.tight_layout()

# Save accuracy plot
accuracy_fig_path = os.path.join(os.getcwd(), "svm_rfe_accuracy.png")
plt.savefig(accuracy_fig_path, dpi=300, bbox_inches='tight')
print(f"Accuracy plot saved to: {accuracy_fig_path}")

accuracy_pdf_path = os.path.join(os.getcwd(), "svm_rfe_accuracy.pdf")
plt.savefig(accuracy_pdf_path, format='pdf', dpi=300, bbox_inches='tight')
print(f"Accuracy plot saved to PDF: {accuracy_pdf_path}")

plt.show()

# ==================== Train Final Model with Optimal Features ====================
print("\nTraining final model with optimal number of features...")

# Select only the features chosen by RFECV
selected_feature_indices = np.where(selected_features_mask)[0]
selected_feature_names = feature_names[selected_feature_indices]
X_selected = X_scaled[:, selected_features_mask]

# Split data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y, test_size=0.3, random_state=42, stratify=y
)

# Train final SVM model on selected features
final_model = SVC(kernel='linear', probability=True, random_state=42)
final_model.fit(X_train, y_train)

# Evaluate performance on training and test sets
train_accuracy = accuracy_score(y_train, final_model.predict(X_train))
test_accuracy = accuracy_score(y_test, final_model.predict(X_test))

print(f"SVM model performance with {n_features_optimal} optimal features:")
print(f"Training accuracy: {train_accuracy:.4f}")
print(f"Testing accuracy: {test_accuracy:.4f}")

# Display list of selected features
print("\nSelected features:")
for i, feature in enumerate(selected_feature_names):
    print(f"{i+1}. {feature}")

# ==================== Visualize Feature Importance Using SVM Coefficients ====================
# For linear SVM, feature importance can be approximated by absolute coefficient values
if hasattr(final_model, 'coef_'):
    importance = np.abs(final_model.coef_[0])  # Shape: (n_features,)

    # Create DataFrame for selected features and their importance
    importance_df = pd.DataFrame({
        'Feature': selected_feature_names,
        'Importance': importance
    }).sort_values('Importance', ascending=False)

    # Plot feature importance as a horizontal bar chart
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
    plt.title(f'Feature Importance of {n_features_optimal} Selected Features (SVM-RFE)', fontsize=16)
    plt.xlabel('Feature Importance (|SVM Coefficient|)', fontsize=14)
    plt.ylabel('Feature', fontsize=14)
    plt.tight_layout()

    # Save importance plot
    importance_fig_path = os.path.join(os.getcwd(), "svm_rfe_feature_importance.png")
    plt.savefig(importance_fig_path, dpi=300, bbox_inches='tight')
    print(f"Feature importance plot saved to: {importance_fig_path}")

    importance_pdf_path = os.path.join(os.getcwd(), "svm_rfe_feature_importance.pdf")
    plt.savefig(importance_pdf_path, format='pdf', dpi=300, bbox_inches='tight')
    print(f"Feature importance plot saved to PDF: {importance_pdf_path}")

    plt.show()
